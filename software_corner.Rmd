---
title: 'mcvis: Multi-collinearity Visualisation'
author: "Kevin Wang & Samuel Mueller"
date: "`r paste0('Initiated on 2020 Aug 22, compiled on ', format(Sys.time(), '%Y %b %d'))`"
output:
  word_document:
    toc: yes
    toc_depth: '3'
    fig_height: 6
    fig_width: 10
    reference_docx: "template.docx"
  html_document:
    code_folding: show
    fig_height: 6
    fig_width: 10
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options:
  chunk_output_type: console
---

# Introduction

What is the most popular statistical model for a statistician? We think most statisticians would say the linear regression model. Indeed, linear regression is both rich in theory and powerful in practice with many flexible extensions such as generalised linear models and linear mixed models. 

But what about the model limitations? Multicollinearity is perhaps the most obvious limitation of regression models, which can cause the least squares regression to produce unreasonable coefficient estimates and standard errors. 

# Multicollinearity $\neq$ high correlation!

So what is multicollinearity? Formally, it is defined as the phenomenon when a group of predictor variables are exactly or approximately linearly dependent. In other words, if one predictor variable can be approximately expressed as a linear combination of other predictor variables, then we have a situation of multicollinearity. 

**It is a common misconception that multicollinearity equals to high correlation between variables!** Consider the following example where we generate $p = 5$ normally distributed predictor variables. The first predictor is constructed as $X_1 = X_2 + X_3 + X_4$ plus some random noise. Looking at the correlation matrix, we don't necessarily see anything alarming with all correlation coefficient being below 0.6. This might give us the false sense of security that no multicollinearity exist when in fact, the correlation of $X_1$ and the sum $X_2 + X_3 + X_4$ is approximately 0.998, enough to trigger multicollinearity for regression!

```{r}
set.seed(123)
p = 5; n = 50
X = matrix(rnorm(n*p), ncol = p)
X[,1] = X[,2] + X[,3] + X[,4] + rnorm(n, 0, 0.1)
round(cor(X), 2)
cor(X[,1], X[,2] + X[,3] + X[,4])
```

If we blindly fit a linear regression model not knowing of the existence of multicollinearity, then two things can happen. One is with the regression coefficient estimate itself, where we can obtain an estimate of $\beta_1$, the slope parameter corresponding to the first regressor $X_1$, that is opposite in sign to how we generated our response variable, $y$. Another consequence is on the standard error of the coefficient estimate. Looking at the summary output of the "full model" (with all $p = 5$ predictors fitted), we see that the standard error of $\hat{\beta}_1$ is ten times larger than that of the "true model" with only $X_1$ fitted. 

```{r}
y = 1 * X[,1] + rnorm(n)
full_model = lm(y ~ X)
broom::tidy(full_model)
```

```{r}
true_model = lm(y ~ X[,1])
broom::tidy(true_model)
```

# Classic ways of diagnosing multicollinearity

So why do these strange phenomena occur? In least squares regression, both the estimator and its variance depend on the precision matrix $(X^\top X)^{-1}$, where each column $X_j$ of $X$ represents an explanatory variable $X_j$, $j = 0, \dots, p$ if the linear regression model includes an intercept term, and $j = 1, \dots, p$ otherwise. However, when the model design matrix, $X$, is numerically "close" to having linearly dependent columns, as we do have when multicollinearity occurs, the matrix $X^\top X$ is not numerically invertible. Hence, elements of the precision matrix, regression estimators and standard errors start to behave erratically from a numeric perspective. 

One way to "diagnose" multicollinearity is to compute the eigenvalues of the matrix $X^\top X$ and examine the value $\sqrt{\lambda_{max}/\lambda_{min}}$, where $\lambda_{max}$ and $\lambda_{min}$ are the maximum eigenvalue and the minimum eigenvalue, respectively. This ratio is known as the "condition number" of the design matrix and a rule of thumb is that if a condition number is larger than 30, then this indicates a severe case of multicollinearity (see e.g. Belsley et al. (1980, Section 3.2)). In our data example, we do indeed have such a severe case of multicollinearity. 

```{r}
evalues = eigen(t(X) %*% X)$values
(condition_number = sqrt(evalues[1]/evalues[p]))
```

However, if we didn't simulate the data above and we obtained such a large condition number, how would we know which variables are contributing the most to cause multicollinearity? 
After all, the condition number only tells us of the existence of multicollinearity, but not which predictors are causing this. 
Knowing this information may help us to decide if any variables should be dropped or alternative modelling strategies might be needed (e.g. averaging these variables). 

This is where `mcvis` can help!

# Introducting `mcvis`

The recently developed `mcvis` package computes a new statistic called the MC-index which can diagnose multicollinearity. 
The theory behind this MC-index is published in Lin et. al. (2020). Briefly, MC-index is a vector of length matching that of the number of columns of $X$. 
Each element of the MC-index vector is a number between zero and one, connecting the $i$th MC-index with the smallest eigenvalue, $\tau_p = \lambda_{min}$, which measures $X$'s proximity to linear dependence, with the $i$th regression predictor variable. 
A larger MC-index value indicates that a variable is causing more multicollinearity than another variable. 
The computation of the MC-index uses a bootstrap resampling scheme to stabilise estimations. 
The process is simple to implement through the `mcvis` function in the `mcvis` R package which is available on CRAN. 

```{r}
# install.packages("mcvis") or remotes::install_github("kevinwang09/mcvis") for the development version
library(mcvis)
mcvis_result = mcvis(X)
print(mcvis_result)
```

In the output above, we see that $X_1$ with the largest value of the MC-index, is identified as the main variable causing multicollinearity.
We can further visualise this via a (bipartite) graph that shows this connection between $\tau_p$ and the $p$ predictor variables. 
The size and colour of the lines are categorised by the magnitude of the MC-index value. 
This plot gives us a quick indication of the sources of the multicollinearity, ordered by the magnitude.

```{r}
plot(mcvis_result)
```

# Breast cancer

We will illustrate the use of the `mcvis` package using a real example from the `dslabs` data package. The `brca` data contains various important biopsy features for breast cancer cell nuclei. The original data measures features such as the radius for a collection of cell nuclei in a tumour and summarises these into mean, standard error and the worst value. The response variable is a factor with two levels denoting whether a tumour is malignant ("M") or benign ("B"). To simplify interpretations, we will only focus on the first ten predictors, which correspond to the mean measurement of cell nuclei. 

We do expect multicollinearity in this data because measures such as radius, perimeter and area of nuclei should be highly correlated. Performing `mcvis` on this data, we do indeed see that this is the case and we will demonstrate that more than one group of variables are linearly highly correlated. We add some extra plotting parameters for `mcvis` to improve visibility.

```{r, message=FALSE}
library(dslabs)
library(dplyr)
library(ggcorrplot)
data(brca)
x = as.data.frame(brca$x[,1:10])
colnames(x)
```

```{r}
ggcorrplot(cor(x), type = "upper", lab = TRUE)
```

```{r}
(mcvis_result = mcvis::mcvis(x))
plot(mcvis_result, var_max = 5, label_dodge = TRUE)
```

The MC-index identifies two variables as having a "Strong" MC-index value. Therefore, in this case we might be tempted to remove the `perimeter_mean` and `area_mean` from our design matrix as they provide very similar information as `radius_mean`. Once the strongest sources of multicollinearity is addressed, additional groups of variables causing multicollinearity may be identified. We can repeat `mcvis` to identify `concavity_mean` and `concave_pts_mean` as highly correlated variables.

```{r}
x2 = dplyr::select(x, -perimeter_mean, -area_mean)
(mcvis_result2 = mcvis::mcvis(x2))	
plot(mcvis_result2, var_max = 5, label_dodge = TRUE)
```

Repeating `mcvis` again, we can also find that `concave_pts_mean` and `concavity_mean` as another potential source of multicollinearity and as their definition are very similar according to the documentation, we may decide to remove one of these. 

Of course, depending on data context, we might not want to simply remove the collinearity causing variables as we have done here. Some common alternatives include averaging collinear predictors or using models that can decorrelate the predictor variables (e.g. random forest). Nonetheless, the diagnosis of multicollinearity is an important part of choosing which modelling strategy one should take. 

# Conclusion 

The linear regression model is arguably the most powerful tool in statistics. However, using it in practice may require extra checks and practical considerations with one often ignored issue being multicollinearity. The `R` package `mcvis` provides tools that can help statisticians to identify different sources of multicollinearity to better decision making and the application of linear regression model. 

# Reference 

+ Belsley, D. A., Kuh, E. & Welsch. R. E. Regression Diagnostics. Wiley Series in Probability and Statistics. John Wiley & Sons, Inc., 1980.

+ Lin, C., Wang, K. & Mueller, S. mcvis: A new framework for collinearity discovery, diagnostic and visualization. Journal of Computational and Graphical Statistics In Press, (2020). DOI: 10.1080/10618600.2020.1779729
